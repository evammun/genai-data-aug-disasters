# synthetic_tests.py
"""
Synthetic Image Evaluation Module

This module provides functions for evaluating synthetic disaster images generated by
various diffusion models (Stable Diffusion, Black Forest, etc.). The goal is to assess
whether the generated images still preserve the original classification labels (e.g.,
damage severity, disaster type), as determined by large language models (Claude, GPT).

Main Features:
    - Loading CSV data with synthetic images, ensuring file paths are updated if needed.
    - Checking for missing images.
    - Calling LLMs (Claude, GPT) to re-classify each synthetic image.
    - Comparing LLM-predicted labels with the original ground-truth labels.
    - Calculating match rates (accuracy) on a per-task basis and aggregating with pivot tables.
    - Displaying final summary tables including subtotals per task, plus a global ALL row.

Usage:
    - To run a full evaluation pipeline, you can call `evaluate_synthetic_images(...)`
      with a dictionary of diffusion model -> CSV path, sample_size, etc.
    - After generating results, call `analyze_synthetic_results(...)` to produce
      HTML-based pivot tables and identify which model/prompt grouping yields the
      best overall accuracy.
"""

import os
import sys
import pandas as pd
import numpy as np
from tqdm import tqdm

from html import escape
import seaborn as sns
from IPython.display import display, HTML
from collections import defaultdict
import json
import config
from config import LABEL_MAPPINGS, DATA_DIR
import shutil
import io
import os
import shutil
import pandas as pd
from html import escape
import imgkit
from sklearn.metrics import confusion_matrix
from typing import Dict, List
import matplotlib.pyplot as plt

# Extend sys.path to locate project modules one or two directories above.
# Adjust to match your local project structure if needed.
sys.path.append(os.path.join(os.path.dirname(__file__), "..", ".."))

from src.labelling.llm_calls import call_claude_api, call_gpt_api
from config import (
    TASKS,
    LABEL_MAPPINGS,
    SYNTHETIC_DATA_DIR,
    SNS_PALETTE,
    SNS_STYLE,
    SNS_CONTEXT,
    SNS_FONT_SCALE,
)

# Configure Seaborn for consistent aesthetic appearance of plots (if any).
sns.set_style(SNS_STYLE)
sns.set_context(SNS_CONTEXT, font_scale=SNS_FONT_SCALE)
sns.set_palette(SNS_PALETTE)

# Maps diffusion model IDs to subfolder names, e.g. "sd_16" -> "sd_16".
# Used for patching CSV paths to locate image files.
DIFFUSION_MODEL_DIRS = {"sd_16": "sd_16", "sd_35": "sd_35", "bfai": "bf"}

# Maps prompt keys (scene/caption) to simplified grouping labels (naive, structured, etc.).
# This helps in pivoting or grouping prompts by their style or generation approach.
PROMPT_MAP = {
    "scene_naive": "naive",
    "caption_naive": "naive",
    "scene_structured": "structured",
    "caption_structured": "structured",
    "scene_multistage": "multistage",
    "caption_multistage": "multistage",
}

# Labels representing a "refusal" or "empty" classification from an LLM, meaning no valid label was given.
REFUSAL_LABELS = {None, "", "refuse", "refusal", "Refusal"}

# Map internal task names to more readable strings for displaying in tables.
TASK_NAME_MAP = {
    "damage_severity": "Damage Severity",
    "informative": "Informative",
    "humanitarian": "Humanitarian",
    "disaster_types": "Disaster Types",
    "ALL": "ALL",
}


def prettify_label(label: str) -> str:
    """
    Convert underscores or uppercase in a label to Title Case with spaces.

    Example:
        "damage_severity" -> "Damage Severity"

    Parameters
    ----------
    label : str
        Input label that may contain underscores.

    Returns
    -------
    str
        A prettified label, e.g. "Damage Severity". If 'ALL', returns 'ALL'.
    """
    if not label or label.upper() == "ALL":
        return "ALL"
    parts = label.strip().split("_")
    return " ".join(p.capitalize() for p in parts)


def load_synthetic_data(csv_path: str, diffusion_model: str) -> pd.DataFrame:
    """
    Load synthetic image metadata from a CSV file, attempting different delimiters (tab, comma) if necessary.
    Also updates the 'aug_img_path' column to reflect the subfolder structure for the specified diffusion model.

    Parameters
    ----------
    csv_path : str
        Path to the CSV file containing synthetic image information.
    diffusion_model : str
        Key identifying which diffusion model's subfolder should be used for the images.

    Returns
    -------
    pd.DataFrame
        A DataFrame with at least the following columns (created if missing):
        ['damage_severity', 'informative', 'humanitarian', 'disaster_types', 'aug_img_path'].
        The 'aug_img_path' column is patched to point to the appropriate subdirectory in SYNTHETIC_DATA_DIR.
    """
    # Attempt loading with tab delimiter.
    try:
        df = pd.read_csv(csv_path, sep="\t")
        print(f"Successfully loaded tab-delimited file with {len(df)} rows: {csv_path}")
    except Exception as e:
        print(f"Error parsing as tab-delimited: {e}")
        # Fallback to comma.
        try:
            df = pd.read_csv(csv_path)
            print(
                f"Successfully loaded comma-delimited file with {len(df)} rows: {csv_path}"
            )
        except Exception as e2:
            print(f"Error parsing as comma-delimited: {e2}")
            # Fallback to auto-detection.
            try:
                df = pd.read_csv(csv_path, sep=None, engine="python")
                print(f"Auto-detected delimiter, loaded {len(df)} rows: {csv_path}")
            except Exception as e3:
                print(f"All parsing methods failed: {e3}")
                return pd.DataFrame()

    # If the file has 'aug_img_path', rename it with the correct subfolder.
    if "aug_img_path" in df.columns:
        df["filename"] = df["aug_img_path"].apply(
            lambda x: os.path.basename(x) if isinstance(x, str) else None
        )
        subdir = DIFFUSION_MODEL_DIRS.get(diffusion_model, diffusion_model)
        df["aug_img_path"] = df["filename"].apply(
            lambda x: f"{SYNTHETIC_DATA_DIR}/{subdir}/{x}" if x else None
        )

    # Ensure the DataFrame has the required columns, create them if missing.
    required = [
        "damage_severity",
        "informative",
        "humanitarian",
        "disaster_types",
        "aug_img_path",
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        print(f"Warning: missing columns: {missing}")
        for col in missing:
            df[col] = None

    return df


def evaluate_synthetic_images(
    csv_files: dict,
    output_dir: str = ".",
    sample_size: int = None,
    llm_types: list = ["claude", "gpt"],
    head_only: int = None,
) -> tuple:
    """
    High-level wrapper function that:
     1. Loads multiple CSVs (one per diffusion model).
     2. Optionally samples or truncates them.
     3. Calls LLMs (Claude, GPT) on each image to re-classify it.
     4. Compares the new LLM-based labels to the original.
     5. Saves results and summary statistics to CSV.

    Parameters
    ----------
    csv_files : dict
        A dictionary mapping diffusion_model_name -> path to CSV file.
        Example: {"sd_16": "synthetic_sd1.6.csv", "bfai": "synthetic_blackforest.csv"}
    output_dir : str
        The folder where result CSVs and metadata should be saved.
    sample_size : int, optional
        If given, each CSV is randomly sampled to this many rows (useful for quick tests).
    llm_types : list
        A list of LLM identifiers to call for classification, e.g. ["claude", "gpt"].
    head_only : int, optional
        If set, only process the first 'head_only' rows from each CSV (useful for debugging).

    Returns
    -------
    (str, str)
        A tuple of (results_path, failure_stats_path) to the final CSV files generated.
        Results file contains per-image classification outcomes; failure file captures stats
        about missing images or other load issues.
    """
    # Determine naming prefix depending on test mode vs full mode
    if head_only:
        print(f"TEST MODE: only first {head_only} rows of each CSV.")
        prefix = "test_evaluation"
    else:
        prefix = "full_evaluation"

    os.makedirs(output_dir, exist_ok=True)

    # Process images, calling LLMs, building classification matches
    results_df, fail_df = _process_synthetic_images(
        csv_files, sample_size, llm_types, head_only
    )

    # Write main results
    res_path = os.path.join(output_dir, f"{prefix}_results.csv")
    fail_path = os.path.join(output_dir, f"{prefix}_failure_stats.csv")
    results_df.to_csv(res_path, index=False)
    fail_df.to_csv(fail_path, index=False)
    print(f"Saved results: {res_path}")
    print(f"Saved failure stats: {fail_path}")

    # Also store pickles
    results_df.to_pickle(os.path.join(output_dir, f"{prefix}_results.pkl"))
    fail_df.to_pickle(os.path.join(output_dir, f"{prefix}_failure_stats.pkl"))

    # Save metadata JSON with context about the run
    meta = {
        "timestamp": str(pd.Timestamp.now()),
        "diffusion_models": list(csv_files.keys()),
        "llm_types": llm_types,
        "sample_size": sample_size,
        "head_only": head_only,
        "rows_in_results": len(results_df),
    }
    with open(os.path.join(output_dir, f"{prefix}_metadata.json"), "w") as f:
        json.dump(meta, f, indent=2)

    return res_path, fail_path


def _process_synthetic_images(
    csv_files: dict,
    sample_size: int,
    llm_types: list,
    head_only: int,
) -> tuple:
    """
    Internal function to:
     - Loop over each CSV, loading synthetic data and optionally sampling/truncating.
     - For each image, check if the file is present. If missing, record as a failure.
     - Otherwise, call each requested LLM (Claude/GPT) to get classification labels.
     - Compare predicted labels to the original (for each of the four tasks), and store results.

    Parameters
    ----------
    csv_files : dict
        Mapping from diffusion_model -> CSV path.
    sample_size : int
        If not None, randomly sample up to this many rows from each CSV.
    llm_types : list
        Which LLMs to invoke, e.g., ["claude", "gpt"].
    head_only : int
        If not None, only process the first N rows from each CSV.

    Returns
    -------
    (pd.DataFrame, pd.DataFrame)
        The first DataFrame contains row-level classification results, including columns for
        each LLM's predictions. The second DataFrame captures aggregated failure statistics
        (e.g., how many images were missing per prompt key).
    """
    all_rows = []
    fail_stats = {
        "diffusion_model": [],
        "prompt_key": [],
        "source_type": [],
        "llm_model": [],
        "total_images": [],
        "missing_images": [],
        "failure_rate": [],
    }

    # Iterate through each diffusion model's CSV
    for diff_m, csv_path in csv_files.items():
        print(f"\nLoading data for {diff_m}: {csv_path}")
        df = load_synthetic_data(csv_path, diff_m)

        # Optionally limit row count
        if head_only and head_only < len(df):
            df = df.head(head_only)
        if sample_size and len(df) > sample_size:
            df = df.sample(sample_size, random_state=42)

        # Tally structures
        prompt_tot = defaultdict(int)
        prompt_fail = defaultdict(int)
        src_tot = defaultdict(int)
        src_fail = defaultdict(int)
        llm_tot = defaultdict(int)
        llm_fail = defaultdict(int)

        # Evaluate each row
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"Evaluating {diff_m}"):
            image_path = row["aug_img_path"]
            s_type = row.get("source_type", "unknown")
            if s_type == "real":
                p_key = row.get("real_prompt_key", "")
            else:
                p_key = row.get("hypo_prompt_key", "")
            l_model = row.get("model_name", "unknown")

            # Increment counters
            prompt_tot[p_key] += 1
            src_tot[s_type] += 1
            llm_tot[l_model] += 1

            # If the image file is missing or invalid, record a fail
            if not isinstance(image_path, str) or not os.path.exists(image_path):
                prompt_fail[p_key] += 1
                src_fail[s_type] += 1
                llm_fail[l_model] += 1

                rdict = {
                    "diffusion_model": diff_m,
                    "source_type": s_type,
                    "prompt_key": p_key,
                    "llm_model": l_model,
                    "image_path": image_path,
                    "image_missing": True,
                }
                # Keep original labels for reference
                for t in TASKS:
                    rdict[f"original_{t}"] = row.get(t, None)
                # LLM predictions are None if missing
                for llm in llm_types:
                    for t in TASKS:
                        rdict[f"{llm}_{t}"] = None
                all_rows.append(rdict)
                continue

            # Image is present => call LLMs
            rdict = {
                "diffusion_model": diff_m,
                "source_type": s_type,
                "prompt_key": p_key,
                "llm_model": l_model,
                "image_path": image_path,
                "image_missing": False,
            }
            # Store original labels
            for t in TASKS:
                rdict[f"original_{t}"] = row.get(t, None)

            # Claude
            if "claude" in llm_types:
                claude_labels, claude_fail = call_claude_api(image_path)
                if claude_fail:
                    # If fail, set predicted to None
                    for t in TASKS:
                        rdict[f"claude_{t}"] = None
                    rdict["claude_fail_reason"] = claude_fail.get("reason", "")
                else:
                    for t in TASKS:
                        rdict[f"claude_{t}"] = (
                            claude_labels.get(t, None) if claude_labels else None
                        )

            # GPT
            if "gpt" in llm_types:
                gpt_labels, gpt_fail = call_gpt_api(image_path)
                if gpt_fail:
                    for t in TASKS:
                        rdict[f"gpt_{t}"] = None
                    rdict["gpt_fail_reason"] = gpt_fail.get("reason", "")
                else:
                    for t in TASKS:
                        rdict[f"gpt_{t}"] = (
                            gpt_labels.get(t, None) if gpt_labels else None
                        )

            all_rows.append(rdict)

        # Aggregate partial failure stats for each model
        for k, tot in prompt_tot.items():
            fails = prompt_fail[k]
            fail_stats["diffusion_model"].append(diff_m)
            fail_stats["prompt_key"].append(k)
            fail_stats["source_type"].append("all")
            fail_stats["llm_model"].append("all")
            fail_stats["total_images"].append(tot)
            fail_stats["missing_images"].append(fails)
            fail_stats["failure_rate"].append(100 * fails / tot if tot > 0 else 0)

        for k, tot in src_tot.items():
            fails = src_fail[k]
            fail_stats["diffusion_model"].append(diff_m)
            fail_stats["prompt_key"].append("all")
            fail_stats["source_type"].append(k)
            fail_stats["llm_model"].append("all")
            fail_stats["total_images"].append(tot)
            fail_stats["missing_images"].append(fails)
            fail_stats["failure_rate"].append(100 * fails / tot if tot > 0 else 0)

        for k, tot in llm_tot.items():
            fails = llm_fail[k]
            fail_stats["diffusion_model"].append(diff_m)
            fail_stats["prompt_key"].append("all")
            fail_stats["source_type"].append("all")
            fail_stats["llm_model"].append(k)
            fail_stats["total_images"].append(tot)
            fail_stats["missing_images"].append(fails)
            fail_stats["failure_rate"].append(100 * fails / tot if tot > 0 else 0)

    # Create final DataFrames
    results_df = pd.DataFrame(all_rows)
    fail_df = pd.DataFrame(fail_stats)

    # Mark whether each LLM's predicted label matched the original
    for llm in llm_types:
        for t in TASKS:
            match_col = f"{llm}_{t}_match"
            pred_col = f"{llm}_{t}"
            orig_col = f"original_{t}"
            results_df[match_col] = np.nan
            valid_mask = (
                ~results_df["image_missing"]
                & results_df[pred_col].notna()
                & results_df[orig_col].notna()
            )
            results_df.loc[valid_mask, match_col] = (
                results_df.loc[valid_mask, pred_col]
                == results_df.loc[valid_mask, orig_col]
            )

    return results_df, fail_df


def _expand_taskwise(df: pd.DataFrame) -> pd.DataFrame:
    """
    Convert the row-level results DataFrame (one row per image) into a more granular format:
    one row per image-task pair. This allows calculation of average accuracy for each (image, task)
    across the chosen LLMs that successfully predicted a label (i.e., ignoring refusals).

    The formula for 'accuracy_taskwise' is (# LLMs that matched original) / (# LLMs that gave a
    non-refusal label). If no LLM gave a valid label, that row is excluded.

    Parameters
    ----------
    df : pd.DataFrame
        The results DataFrame returned by _process_synthetic_images, with columns like "claude_<task>_match",
        "gpt_<task>_match", etc.

    Returns
    -------
    pd.DataFrame
        A DataFrame where each row corresponds to a single (image, task). Contains columns:
        ['diffusion_model', 'prompt_group', 'llm_model', 'task', 'original_label', 'accuracy_taskwise'].
    """
    rows = []
    for _, row in df.iterrows():
        # Skip if the image is missing.
        if row.get("image_missing", True):
            continue

        raw_prompt = row.get("prompt_key", "")
        prompt_group = PROMPT_MAP.get(raw_prompt, raw_prompt)  # unify keys

        # For each task, check how many LLMs provided a valid label
        for task in TASKS:
            orig_label = row.get(f"original_{task}", None)
            if pd.isna(orig_label) or orig_label in REFUSAL_LABELS:
                # If the original label is invalid or a refusal, skip.
                continue

            matches = 0
            possible = 0
            for llm in ["claude", "gpt"]:
                pred_val = row.get(f"{llm}_{task}", None)
                match_val = row.get(f"{llm}_{task}_match", False)
                # If this LLM gave a real classification (non-refusal, non-NA)...
                if pred_val not in REFUSAL_LABELS and pd.notna(pred_val):
                    possible += 1
                    if match_val is True:
                        matches += 1

            # If no LLM produced a valid label for this task, exclude the row altogether.
            if possible == 0:
                continue

            acc = matches / possible
            rows.append(
                {
                    "diffusion_model": row.get("diffusion_model", ""),
                    "prompt_group": prompt_group,
                    "llm_model": row.get("llm_model", ""),
                    "task": task,
                    "original_label": str(orig_label),
                    "accuracy_taskwise": acc,
                }
            )

    return pd.DataFrame(rows)


def _make_pivot_table(df: pd.DataFrame, group_col: str) -> pd.DataFrame:
    """
    Create a pivot table summarizing average accuracy by (task, original_label) vs. the group_col.

    Also adds "ALL" subtotal rows:
      1) (task, "ALL") for each task -> average accuracy across that task's classes
      2) ("ALL", "ALL") -> global average across all tasks/classes

    Parameters
    ----------
    df : pd.DataFrame
        Output of _expand_taskwise, containing rows for each (image, task).
    group_col : str
        The column name to pivot on (e.g., 'diffusion_model', 'prompt_group', 'llm_model').

    Returns
    -------
    pd.DataFrame
        A pivoted DataFrame indexed by (task, original_label), with group_col categories as columns.
        Each cell shows the mean accuracy in percentage.
    """
    if df.empty:
        return pd.DataFrame()

    # Compute mean(accuracy_taskwise * 100) for each group
    grp = (
        df.groupby(["task", "original_label", group_col])["accuracy_taskwise"]
        .mean()
        .reset_index()
    )
    grp["accuracy_taskwise"] *= 100

    # Pivot so that rows = (task, original_label), columns = group_col, values = average accuracy
    pvt = grp.pivot_table(
        index=["task", "original_label"],
        columns=group_col,
        values="accuracy_taskwise",
        aggfunc="mean",
    )
    if pvt.empty:
        return pvt

    # Insert (task, ALL) sub-totals
    tasks_in_index = sorted(set(x[0] for x in pvt.index if x[0] != "ALL"))
    for t in tasks_in_index:
        classes_for_task = [x for x in pvt.index if x[0] == t and x[1] != "ALL"]
        if not classes_for_task:
            continue
        sub = pvt.loc[classes_for_task].mean(axis=0)
        pvt.loc[(t, "ALL"), :] = sub

    # Insert a global (ALL, ALL) row
    global_row = pvt.mean(axis=0)
    pvt.loc[("ALL", "ALL"), :] = global_row
    return pvt


def _display_custom_table(pvt: pd.DataFrame, step_title: str) -> None:
    """
    Render a pivot table in HTML with custom styling:
     - Each task appears in a separate block with one row per class.
     - A sub-total row (task, ALL) is inserted.
     - A final global row (ALL, ALL) at the bottom.

    Parameters
    ----------
    pvt : pd.DataFrame
        The pivot table produced by _make_pivot_table.
    step_title : str
        Heading displayed above the table to label it.
    """
    if pvt.empty:
        print(f"No data for {step_title}")
        return

    # Convert the MultiIndex into a sorted list we can iterate over
    index_tuples = list(pvt.index)
    global_present = ("ALL", "ALL") in index_tuples
    if global_present:
        index_tuples.remove(("ALL", "ALL"))

    tasks = sorted(set(t for (t, _) in index_tuples if t != "ALL"))
    columns = list(pvt.columns)

    html = f"""
    <h4 style="margin-top:1em;">{step_title}</h4>
    <table style="border-collapse: collapse; width:100%;">
      <tr style="background-color:#333; color:white;">
        <th style="text-align:left; padding:6px;">Task/Class</th>
    """
    # Column headers
    for col in columns:
        html += f'<th style="text-align:center; padding:6px;">{prettify_label(str(col))}</th>'
    html += "</tr>"

    row_toggle = False

    # For each task, display the classes plus a sub-total row.
    for task in tasks:
        classes = []
        for idx in index_tuples:
            if idx[0] == task and idx[1] != "ALL":
                classes.append(idx[1])
        has_subtotal = (task, "ALL") in pvt.index

        for cls_label in classes:
            row_bg = "#2e2e2e" if row_toggle else "#222"
            row_toggle = not row_toggle
            label_str = f"  {prettify_label(cls_label)}"
            row_html = f'<tr style="background-color:{row_bg}; color:white;">'
            row_html += f'<td style="text-align:left; padding:6px;">{label_str}</td>'

            for c in columns:
                val = (
                    pvt.loc[(task, cls_label), c]
                    if (task, cls_label) in pvt.index
                    else np.nan
                )
                cell_str = f"{val:.1f}" if pd.notna(val) else ""
                row_html += (
                    f'<td style="text-align:center; padding:6px;">{cell_str}</td>'
                )
            row_html += "</tr>"
            html += row_html

        # Insert sub-total row for the task if present
        if has_subtotal:
            row_bg = "#111" if row_toggle else "#222"
            row_toggle = not row_toggle
            task_pretty = TASK_NAME_MAP.get(task, prettify_label(task))
            row_html = f'<tr style="background-color:{row_bg}; color:white;">'
            row_html += f'<td style="font-weight:bold; text-align:left; padding:6px;">{task_pretty}</td>'
            for c in columns:
                val = pvt.loc[(task, "ALL"), c]
                cell_str = f"{val:.1f}" if pd.notna(val) else ""
                row_html += f'<td style="text-align:center; padding:6px; font-weight:bold;">{cell_str}</td>'
            row_html += "</tr>"
            html += row_html

    # Finally, the global (ALL, ALL) row if present
    if global_present:
        row_bg = "#111" if row_toggle else "#222"
        row_html = f'<tr style="background-color:{row_bg}; color:white;">'
        row_html += (
            '<td style="font-weight:bold; text-align:left; padding:6px;">ALL</td>'
        )
        for c in columns:
            val = pvt.loc[("ALL", "ALL"), c]
            cell_str = f"{val:.1f}" if pd.notna(val) else ""
            row_html += f'<td style="text-align:center; padding:6px; font-weight:bold;">{cell_str}</td>'
        row_html += "</tr>"
        html += row_html

    html += "</table>"
    display(HTML(html))


def analyze_synthetic_results(
    results_path: str,
    failure_stats_path: str = None,
    output_dir: str = None,
    llm_types: list = ["claude", "gpt"],
) -> dict:
    """
    Conduct a multi-step analysis of synthetic evaluation results. The analysis:
      1) Loads the results from CSV or PKL, dropping any rows where the image is missing.
      2) Converts the data to a per-task format (one row per (image, task)).
      3) Produces pivot tables broken down by diffusion_model, prompt_group, and llm_model in sequence.
         After each step, filters the DataFrame to keep only the best group (the one with the highest overall accuracy).
      4) Displays each pivot in a stylized HTML table in the notebook.
      5) Returns a dictionary containing references to final chosen subsets and best group IDs.

    Parameters
    ----------
    results_path : str
        File path to the evaluation results (CSV or PKL) from `evaluate_synthetic_images`.
    failure_stats_path : str, optional
        Path to a file containing failure stats. Not used in the final analysis logic,
        but loaded for reference if it exists.
    output_dir : str, optional
        Currently unused in this function. Could be used for saving plots or additional artifacts.
    llm_types : list, optional
        The LLM keys we want to consider (e.g. ["claude", "gpt"]). In practice, used by intermediate expansions.

    Returns
    -------
    dict
        Dictionary storing the best-chosen diffusion model, prompt group, llm model, plus
        a key "final_df" containing the final filtered DataFrame after all steps.
    """
    print(f"Analyze results from: {results_path}")

    # Load the main results
    if results_path.endswith(".pkl"):
        df = pd.read_pickle(results_path)
        print(f"Loaded pkl with {len(df)} rows")
    else:
        df = pd.read_csv(results_path)
        print(f"Loaded csv with {len(df)} rows")

    # Optionally load the failure stats
    if failure_stats_path and os.path.exists(failure_stats_path):
        print(f"Loaded failure stats from {failure_stats_path}")
    else:
        print("No failure stats found/used.")

    # Filter out missing images
    df = df[~df["image_missing"]].copy()
    print(f"Non-missing images: {len(df)}")

    # Convert to per-task rows
    expanded = _expand_taskwise(df)
    if expanded.empty:
        print("All data is invalid or missing.")
        return {}

    # We'll store some metrics about best picks
    metrics = {}

    # ----- STEP 1: Compare by diffusion_model -----
    expanded.rename(columns={"prompt_key": "prompt_group"}, inplace=True)

    if "diffusion_model" in expanded and expanded["diffusion_model"].nunique() > 1:
        print("\n===== STEP 1: diffusion_model =====")
        pvt = _make_pivot_table(expanded, "diffusion_model")
        _display_custom_table(pvt, "Diffusion Model Comparison")
        if ("ALL", "ALL") in pvt.index:
            best_col = pvt.loc[("ALL", "ALL")].idxmax()
            best_val = pvt.loc[("ALL", "ALL")].max()
            print(
                f"Best diffusion model: {prettify_label(best_col)} => {best_val:.1f}%"
            )
            # Filter data to keep only that best model
            expanded = expanded[expanded["diffusion_model"] == best_col].copy()
            metrics["best_diffusion_model"] = best_col
            metrics["best_diff_acc"] = best_val

    # ----- STEP 2: Compare by prompt_group -----
    if "prompt_group" in expanded and expanded["prompt_group"].nunique() > 1:
        print("\n===== STEP 2: prompt_group =====")
        pvt = _make_pivot_table(expanded, "prompt_group")
        _display_custom_table(pvt, "Prompt Group Comparison")
        if ("ALL", "ALL") in pvt.index:
            best_col = pvt.loc[("ALL", "ALL")].idxmax()
            best_val = pvt.loc[("ALL", "ALL")].max()
            print(f"Best prompt group: {prettify_label(best_col)} => {best_val:.1f}%")
            expanded = expanded[expanded["prompt_group"] == best_col].copy()
            metrics["best_prompt_group"] = best_col
            metrics["best_prompt_acc"] = best_val

    # ----- STEP 3: Compare by llm_model -----
    if "llm_model" in expanded and expanded["llm_model"].nunique() > 1:
        print("\n===== STEP 3: llm_model =====")
        pvt = _make_pivot_table(expanded, "llm_model")
        _display_custom_table(pvt, "LLM Model Comparison")
        if ("ALL", "ALL") in pvt.index:
            best_col = pvt.loc[("ALL", "ALL")].idxmax()
            best_val = pvt.loc[("ALL", "ALL")].max()
            print(f"Best LLM Model: {prettify_label(best_col)} => {best_val:.1f}%")
            expanded = expanded[expanded["llm_model"] == best_col].copy()
            metrics["best_llm_model"] = best_col
            metrics["best_llm_acc"] = best_val

    # Final summary
    print("\n===== FINAL SELECTION =====")
    dmod = metrics.get("best_diffusion_model", "(none)")
    pgrp = metrics.get("best_prompt_group", "(none)")
    lmod = metrics.get("best_llm_model", "(none)")
    print(f" - Diffusion: {dmod}")
    print(f" - Prompt: {pgrp}")
    print(f" - LLM: {lmod}")

    metrics["final_df"] = expanded
    return metrics


DATA_DIR = config.DATA_DIR


LABEL_MAPPINGS = config.LABEL_MAPPINGS


def plot_confusion_matrices(
    csv: str,
    llm_models: list,
    diffusion_model: str = None,
    prompt_key: str = None,
) -> None:
    """
    Load the given CSV of evaluation results, filter by diffusion_model and/or prompt_key,
    and produce 4 confusion matrices (damage_severity, informative, humanitarian,
    disaster_types) for each specified LLM. Directly shows the plots in the notebook.

    Parameters
    ----------
    csv : str
        File path to the CSV containing columns:
          - "diffusion_model", "prompt_key", "original_damage_severity", etc.
        plus predicted columns like "gpt_damage_severity", "claude_damage_severity", etc.
    llm_models : list
        List of full LLM model names (e.g. ["claude-3-5-haiku-20241022", "gpt-4o-2024-08-06"])
        for which to generate confusion matrices. These should match the values in the
        llm_model column of the CSV.
    diffusion_model : str, optional
        If provided, filter rows to df["diffusion_model"] == diffusion_model.
    prompt_key : str, optional
        If provided, filter rows to df["prompt_key"] == prompt_key.

    Returns
    -------
    None
        This function prints the confusion matrix plots directly.
    """
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import confusion_matrix
    from config import LABEL_MAPPINGS

    # -------------------------------------------------------------------------
    # 1) Load the CSV into a DataFrame
    # -------------------------------------------------------------------------
    df = pd.read_csv(csv)
    print(f"Loaded {len(df)} rows from {csv}")

    # -------------------------------------------------------------------------
    # 2) Optional filtering by diffusion model / prompt key
    # -------------------------------------------------------------------------
    if diffusion_model is not None:
        df = df[df["diffusion_model"] == diffusion_model].copy()

    if prompt_key is not None:
        df = df[df["prompt_key"] == prompt_key].copy()

    if df.empty:
        print("[Warning] After filtering, the DataFrame is empty. No plots to show.")
        return

    # -------------------------------------------------------------------------
    # 3) Filter by LLM models
    # -------------------------------------------------------------------------
    if llm_models:
        df = df[df["llm_model"].isin(llm_models)].copy()
        print(f"Filtered by llm_models={llm_models}; {len(df)} rows remain.")

        if df.empty:
            print(
                "[Warning] After filtering by LLM models, the DataFrame is empty. No plots to show."
            )
            return

    # -------------------------------------------------------------------------
    # 4) We will always produce confusion matrices for these four tasks
    # -------------------------------------------------------------------------
    tasks = ["disaster_types", "informative", "humanitarian", "damage_severity"]

    # Titles to display on the subplots
    task_titles = {
        "disaster_types": "Disaster Types",
        "informative": "Informative",
        "humanitarian": "Humanitarian",
        "damage_severity": "Damage Severity",
    }

    # -------------------------------------------------------------------------
    # 5) Shorter label names for visualization
    # -------------------------------------------------------------------------
    # Define shorter display names for each label
    LABEL_DISPLAY_NAMES = {
        "damage_severity": {
            "little_or_none": "none",
            "mild": "mild",
            "severe": "severe",
        },
        "informative": {"not_informative": "not inf", "informative": "inf"},
        "humanitarian": {
            "not_humanitarian": "not hum",
            "affected_injured_or_dead_people": "injured",
            "infrastructure_and_utility_damage": "infra",
            "rescue_volunteering_or_donation_effort": "rescue",
        },
        "disaster_types": {
            "not_disaster": "none",
            "earthquake": "quake",
            "fire": "fire",
            "flood": "flood",
            "hurricane": "hurr.",
            "landslide": "land.",
            "other_disaster": "other",
        },
    }

    # -------------------------------------------------------------------------
    # 6) Define remapping for non-standard values
    # -------------------------------------------------------------------------
    NON_STANDARD_MAPPINGS = {
        "damage_severity": {
            "not_disaster": "little_or_none",
            "not_informative": "little_or_none",
            "not_applicable": "little_or_none",
        },
        "disaster_types": {"tornado": "other_disaster", "avalanche": "other_disaster"},
    }

    # -------------------------------------------------------------------------
    # 7) Set up the figure grid:
    #    - One row per LLM model
    #    - Four columns (one for each task)
    # -------------------------------------------------------------------------
    n_rows = len(llm_models)
    n_cols = len(tasks)

    fig, axes = plt.subplots(
        n_rows, n_cols, figsize=(5.5 * n_cols, 4.5 * n_rows), squeeze=False
    )

    # -------------------------------------------------------------------------
    # 8) For each LLM, produce the 4 confusion matrices
    # -------------------------------------------------------------------------
    for row_idx, llm_model in enumerate(llm_models):
        # Get short name for the LLM model for display
        if "claude-3-5-haiku" in llm_model:
            llm_display = "Claude Haiku"
            llm_prefix = "claude"
        elif "claude-3-7-sonnet" in llm_model:
            llm_display = "Claude Sonnet"
            llm_prefix = "claude"
        elif "gpt-4o" in llm_model:
            llm_display = "GPT-4o"
            llm_prefix = "gpt"
        else:
            llm_display = llm_model
            llm_prefix = llm_model.split("-")[0]  # Use first part as prefix

        # Filter data for this specific LLM model
        llm_df = df[df["llm_model"] == llm_model].copy()

        if llm_df.empty:
            continue

        for col_idx, task in enumerate(tasks):
            ax = axes[row_idx, col_idx]

            # Identify the columns for ground-truth and predicted labels
            true_col = f"original_{task}"
            pred_col = f"{llm_prefix}_{task}"  # Use the prefix for column names

            # If columns are missing, skip gracefully
            if true_col not in llm_df.columns or pred_col not in llm_df.columns:
                ax.text(
                    0.5, 0.5, "Missing columns", ha="center", va="center", fontsize=12
                )
                ax.set_title(f"{llm_display}: {task_titles.get(task, task)} (No Data)")
                ax.axis("off")
                continue

            try:
                # Create a working copy of the dataframe for this task
                task_df = llm_df.copy()

                # Apply non-standard label mapping if there are any for this task
                if task in NON_STANDARD_MAPPINGS:
                    # Map non-standard values in both original and predicted columns
                    for non_standard, standard in NON_STANDARD_MAPPINGS[task].items():
                        # Map in true column
                        mask = task_df[true_col] == non_standard
                        if mask.sum() > 0:
                            task_df.loc[mask, true_col] = standard

                        # Map in predicted column
                        mask = task_df[pred_col] == non_standard
                        if mask.sum() > 0:
                            task_df.loc[mask, pred_col] = standard

                # Get valid labels for this task
                # Handle both formats - if LABEL_MAPPINGS is a dict with integer keys
                if isinstance(LABEL_MAPPINGS[task], dict):
                    # Extract just the values from the dictionary
                    label_list = list(LABEL_MAPPINGS[task].values())
                else:
                    # If it's already a list, use it directly
                    label_list = LABEL_MAPPINGS[task]

                # Filter out rows with NaN values or values not in the valid label list
                mask = task_df[true_col].notna() & task_df[pred_col].notna()
                mask = (
                    mask
                    & task_df[true_col].isin(label_list)
                    & task_df[pred_col].isin(label_list)
                )

                if mask.sum() == 0:
                    ax.text(
                        0.5,
                        0.5,
                        "No valid data points",
                        ha="center",
                        va="center",
                        fontsize=12,
                    )
                    ax.set_title(
                        f"{llm_display}: {task_titles.get(task, task)} (No Valid Data)"
                    )
                    ax.axis("off")
                    continue

                # Use only the filtered rows
                y_true = task_df.loc[mask, true_col]
                y_pred = task_df.loc[mask, pred_col]

                # Build confusion matrix with the valid data
                cm = confusion_matrix(y_true, y_pred, labels=label_list)

                # Normalise each row
                cm_sum_per_row = cm.sum(axis=1)[:, np.newaxis]
                cm_normalized = cm.astype("float") / np.clip(cm_sum_per_row, 1e-9, None)

                # Build a custom "positive diagonal, negative off-diagonal" array
                custom_norm = np.zeros_like(cm_normalized)
                for i in range(len(custom_norm)):
                    for j in range(len(custom_norm[i])):
                        if i == j:
                            custom_norm[i, j] = cm_normalized[i, j]
                        else:
                            custom_norm[i, j] = -cm_normalized[i, j]

                # Build the annotation strings
                # E.g. 0.123 -> .12, 1 -> 1.00, 0 -> 0
                annotations = []
                for row in cm_normalized:
                    row_ann = []
                    for x in row:
                        if 0 < x < 1:
                            frac_str = f"{x:.2f}"  # e.g. 0.12
                            if frac_str.startswith("0."):
                                frac_str = frac_str[1:]  # remove leading 0 => .12
                            row_ann.append(frac_str)
                        elif x == 1:
                            row_ann.append("1.00")
                        else:
                            row_ann.append("0")
                    annotations.append(row_ann)

                # Get display labels (shorter versions) for the heatmap
                display_labels = []
                for label in label_list:
                    if label in LABEL_DISPLAY_NAMES[task]:
                        display_labels.append(LABEL_DISPLAY_NAMES[task][label])
                    else:
                        # If no display name is defined, use the original label
                        display_labels.append(label)

                # Plot the heatmap
                sns.heatmap(
                    custom_norm,
                    annot=annotations,
                    fmt="",
                    cmap=sns.diverging_palette(10, 240, s=100, l=40, n=9),
                    xticklabels=display_labels,  # Use shorter display labels
                    yticklabels=display_labels,  # Use shorter display labels
                    ax=ax,
                    cbar=False,
                    annot_kws={"fontsize": 12},
                    vmin=-0.5,
                    center=0,
                    vmax=1.0,
                )

                # Labels and title
                ax.set_title(f"{llm_display}: {task_titles[task]}", fontsize=12)
                ax.set_xlabel("Predicted", fontsize=11)
                ax.set_ylabel("True", fontsize=11)
                ax.set_xticklabels(
                    ax.get_xticklabels(), rotation=30, ha="right", fontsize=10
                )
                ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=10)

            except Exception as e:
                ax.text(
                    0.5,
                    0.5,
                    f"Error: {str(e)}",
                    ha="center",
                    va="center",
                    fontsize=10,
                    wrap=True,
                )
                ax.set_title(f"{llm_display}: {task_titles.get(task, task)} (Error)")
                ax.axis("off")

    plt.tight_layout()

    # -------------------------------------------------------------------------
    # 9) Print the figure in the notebook
    # -------------------------------------------------------------------------
    plt.show()


import os
import shutil
import pandas as pd
from html import escape
import imgkit
from config import LABEL_MAPPINGS, DATA_DIR, SEED


import os
import shutil
import pandas as pd
from html import escape
import imgkit
from typing import Union
from config import LABEL_MAPPINGS, DATA_DIR, SEED


def create_synthetic_test_audit(
    input_data: Union[pd.DataFrame, str],
    output_dir: str = "../../src/augmentation/Image outputs",
    max_examples_real: int = 1,
    max_examples_hypo: int = 1,
    verbose: bool = False,  # Set to False by default for minimal output
) -> None:
    """
    For each label combo, randomly selects up to `max_examples_real` real rows
    and `max_examples_hypo` hypothetical rows, building a more compact table
    layout. The table top row left-aligns "From image" or "From labels," and
    each block below has a sub-caption ("Original image," "LLM caption," etc.).
    Paragraph indentation is removed from the textual captions.

    Args:
        input_data (Union[pd.DataFrame, str]):
            Either a DataFrame containing the necessary columns, or a path to a CSV file.
            Must contain columns:
                ["image_path", "damage_severity", "informative", "humanitarian",
                 "disaster_types", "caption_text", "source_type", "aug_img_path"]
        output_dir (str):
            Directory for PNG files (emptied each run).
        max_examples_real (int):
            Number of real examples per label combination.
        max_examples_hypo (int):
            Number of hypothetical examples per label combination.
        verbose (bool):
            Whether to print detailed progress information.

    Returns:
        None. Creates .png files in `output_dir`.

    Metadata:
        Maintainer: [Your Name]
        Created: 2025-02-28
        Dependencies: config.py (for SEED), LABEL_MAPPINGS, DATA_DIR
    """
    # If input is a string (file path), read the CSV with special handling for newlines in fields
    if isinstance(input_data, str):
        csv_path = input_data
        print(f"Reading CSV from: {csv_path}")
        
        # Use the approach that's working - basic reading with auto-detection
        try:
            df = pd.read_csv(csv_path, sep=None, engine='python')
            print(f"Loaded {len(df)} rows with auto-detection")
        except Exception as e:
            if verbose:
                print(f"Error reading CSV: {str(e)}")
            print("Creating an empty DataFrame as fallback")
            df = pd.DataFrame()
    else:
        # Input is already a DataFrame
        df = input_data
    
    # Check if DataFrame is empty
    if df.empty:
        print("ERROR: DataFrame is empty - cannot proceed")
        return
    
    # Print column information only in verbose mode
    if verbose:
        print("\nColumns in dataframe:")
        for i, col in enumerate(df.columns):
            print(f"  {i}: {col}")
    
    # Clean column names if they have quotes or asterisks
    column_cleaning = {}
    for col in df.columns:
        clean_name = col
        if col.startswith('"') and col.endswith('"'):
            clean_name = col.strip('"')
        if '**' in col:
            clean_name = col.replace('*', '')
        if clean_name != col:
            column_cleaning[col] = clean_name
    
    if column_cleaning and verbose:
        df = df.rename(columns=column_cleaning)
        print("\nCleaned column names:")
        for i, col in enumerate(df.columns):
            print(f"  {i}: {col}")
    elif column_cleaning:
        df = df.rename(columns=column_cleaning)
    
    # Check for required columns
    required_columns = [
        "image_path", "damage_severity", "informative", "humanitarian",
        "disaster_types", "caption_text", "source_type", "aug_img_path"
    ]
    
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"ERROR: Missing required columns: {missing_columns}")
        return

    def get_label(task: str, value) -> str:
        """Integer -> label, else string pass-through."""
        if isinstance(value, int):
            return LABEL_MAPPINGS[task].get(value, f"Unknown({value})")
        return str(value)

    def shorten_label(label: str) -> str:
        """Shorten label for filenames."""
        overrides = {
            "little_or_none": "none",
            "informative": "inf",
            "not_informative": "notinf",
            "earthquake": "eart",
        }
        return overrides.get(label, label[:4].lower())

    # ---------------------------------------------
    # Real row table: "From image" in top-left col
    # ---------------------------------------------
    def make_real_table_html(row) -> str:
        caption_html = escape(str(row["caption_text"]))
        
        # Fix paths for imgkit - check if paths are relative or absolute
        orig_path = str(row["image_path"])
        if not os.path.isabs(orig_path):
            orig_path = os.path.join(DATA_DIR, orig_path)
        
        synth_path = str(row["aug_img_path"])
        # Make sure synthetic image path is absolute
        if not os.path.isabs(synth_path):
            synth_path = os.path.join(DATA_DIR, synth_path)
        
        # Check if files exist (silently replace with placeholders if missing)
        if not os.path.exists(orig_path):
            orig_path = "https://via.placeholder.com/200x150?text=Image+Not+Found"
        
        if not os.path.exists(synth_path):
            synth_path = "https://via.placeholder.com/200x150?text=Image+Not+Found"
        
        orig_path = escape(orig_path)
        synth_path = escape(synth_path)

        return f"""
        <table style="border:1px solid #ccc; border-collapse:collapse; margin:8px 0; table-layout:fixed; width:1000px;">
          <!-- Header row: 'From image' left-aligned, other two columns blank -->
          <tr style="border-bottom:1px solid #ccc;">
            <th style="width:220px; text-align:left; padding:6px;">From image</th>
            <th style="width:480px; text-align:left; padding:6px;"></th>
            <th style="width:220px; text-align:left; padding:6px;"></th>
          </tr>
          <!-- Data row -->
          <tr style="vertical-align:top;">
            <!-- Original image -->
            <td style="padding:6px; text-align:center;">
              <img src="file://{orig_path}" alt="Original image" style="max-width:200px; border:1px solid #999;"/>
              <div style="font-size:0.9em; margin-top:4px; font-style:italic;">Original image</div>
            </td>
            <!-- LLM text -->
            <td style="padding:6px;">
              <div style="border:1px solid #ccc; padding:6px; white-space:pre-wrap; margin:0; text-indent:0;">
                {caption_html}
              </div>
              <div style="font-size:0.9em; margin-top:4px; font-style:italic;">LLM caption</div>
            </td>
            <!-- Synthetic image -->
            <td style="padding:6px; text-align:center;">
              <img src="file://{synth_path}" alt="Synthetic image" style="max-width:200px; border:1px solid #999;"/>
              <div style="font-size:0.9em; margin-top:4px; font-style:italic;">Synthetic image</div>
            </td>
          </tr>
        </table>
        """

    # ----------------------------------------------------
    # Hypothetical row: "From labels" in top-left col (2 columns)
    # ----------------------------------------------------
    def make_hypo_table_html(row) -> str:
        caption_html = escape(str(row["caption_text"]))
        
        # Fix path for synthetic image
        synth_path = str(row["aug_img_path"])
        # Make sure synthetic image path is absolute
        if not os.path.isabs(synth_path):
            synth_path = os.path.join(DATA_DIR, synth_path)
        
        # Check if file exists (silently use placeholder if missing)
        if not os.path.exists(synth_path):
            synth_path = "https://via.placeholder.com/200x150?text=Image+Not+Found"
        
        synth_path = escape(synth_path)

        return f"""
        <table style="border:1px solid #ccc; border-collapse:collapse; margin:8px 0; table-layout:fixed; width:1000px;">
          <!-- Header row: 'From labels' left-aligned, other col blank -->
          <tr style="border-bottom:1px solid #ccc;">
            <th style="width:780px; text-align:left; padding:6px;">From labels</th>
            <th style="width:220px; text-align:left; padding:6px;"></th>
          </tr>
          <!-- Data row -->
          <tr style="vertical-align:top;">
            <!-- LLM text -->
            <td style="padding:6px;">
              <div style="border:1px solid #ccc; padding:6px; white-space:pre-wrap; margin:0; text-indent:0;">
                {caption_html}
              </div>
              <div style="font-size:0.9em; margin-top:4px; font-style:italic;">LLM caption</div>
            </td>
            <!-- Synthetic image -->
            <td style="padding:6px; text-align:center;">
              <img src="file://{synth_path}" alt="Synthetic image" style="max-width:200px; border:1px solid #999;"/>
              <div style="font-size:0.9em; margin-top:4px; font-style:italic;">Synthetic image</div>
            </td>
          </tr>
        </table>
        """

    # 1) Clear out the output directory first
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir, exist_ok=True)

    # 2) Group the dataframe by label columns
    group_cols = ["damage_severity", "informative", "humanitarian", "disaster_types"]
    try:
        grouped = df.groupby(group_cols)
    except Exception as e:
        # Silently convert all group columns to string
        for col in group_cols:
            df[col] = df[col].astype(str)
        
        # Try groupby again
        try:
            grouped = df.groupby(group_cols)
        except Exception as e2:
            print("Error: Failed to group data by columns")
            return

    # 3) For each label combination, randomly select rows, build HTML, and convert to PNG
    for (ds_val, inf_val, hum_val, dis_val), group_df in grouped:
        ds_label = get_label("damage_severity", ds_val)
        inf_label = get_label("informative", inf_val)
        hum_label = get_label("humanitarian", hum_val)
        dis_label = get_label("disaster_types", dis_val)

        # Partition real vs hypothetical
        real_subset = group_df[group_df["source_type"] == "real"]
        hypo_subset = group_df[group_df["source_type"] != "real"]

        # Sample with a fixed seed
        if not real_subset.empty:
            real_rows = real_subset.sample(
                n=min(len(real_subset), max_examples_real), random_state=SEED
            )
        else:
            real_rows = pd.DataFrame([])

        if not hypo_subset.empty:
            hypo_rows = hypo_subset.sample(
                n=min(len(hypo_subset), max_examples_hypo), random_state=SEED
            )
        else:
            hypo_rows = pd.DataFrame([])

        # Build the HTML
        html_parts = [f"<div style='border:1px solid #aaa; padding:8px; margin:8px;'>"]
        for _, row in real_rows.iterrows():
            html_parts.append(make_real_table_html(row))
        for _, row in hypo_rows.iterrows():
            html_parts.append(make_hypo_table_html(row))
        html_parts.append("</div>")

        final_html = "".join(html_parts)

        # Construct a filename from the shortened labels
        ds_short = shorten_label(ds_label)
        inf_short = shorten_label(inf_label)
        hum_short = shorten_label(hum_label)
        dis_short = shorten_label(dis_label)
        out_fname = f"{ds_short}_{inf_short}_{hum_short}_{dis_short}.png"
        out_path = os.path.join(output_dir, out_fname)

        # Save HTML (silently)
        html_path = os.path.join(output_dir, f"{ds_short}_{inf_short}_{hum_short}_{dis_short}.html")
        try:
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(final_html)
        except Exception:
            pass  # Silently ignore HTML writing errors

        # Convert HTML to PNG with minimal output
        try:
            # Enhanced options for imgkit
            options = {
                "--quiet": "",
                "--enable-local-file-access": "",
                "--disable-smart-width": "",
                "--width": "1200"
            }
            imgkit.from_string(final_html, out_path, options=options)
        except Exception:
            pass  # Silently ignore PNG conversion errors

import os
import sys
import pandas as pd
import numpy as np
from tqdm import tqdm
from collections import defaultdict
import json

# Import necessary functions from the original synthetic_tests.py
# Assuming these imports would work in your environment
from src.labelling.llm_calls import call_claude_api, call_gpt_api
from config import TASKS, LABEL_MAPPINGS, SYNTHETIC_DATA_DIR

# Constants from the original file
DIFFUSION_MODEL_DIRS = {
    "sd_16": "stable-diffusion-v1-6",
    "sd_35": "sd3.5-medium",
    "bfai": "bfai",
}
REFUSAL_LABELS = {None, "", "refuse", "refusal", "Refusal"}
TASK_NAME_MAP = {
    "damage_severity": "Damage Severity",
    "informative": "Informative",
    "humanitarian": "Humanitarian",
    "disaster_types": "Disaster Types",
    "ALL": "ALL",
}


def call_claude_api_synthetic(image_path):
    """
    Calls Anthropic (Claude) using the SYNTHETIC_EVALUATION_PROMPT for instructions.
    Returns (labels_dict, None) if successful, or (None, fail_record) if something fails.

    The fail_record is a dict with keys:
      {
        "model": "Claude",
        "image_path": <str>,
        "reason": <str>,
        "raw_content": <str>,
      }
    """
    import base64
    import json
    import anthropic
    import re
    from src.labelling.llm_calls import ANTHROPIC_KEY, ANTHROPIC_MODEL, guess_media_type
    from config import SYNTHETIC_EVALUATION_PROMPT

    # 1) Read & encode image
    try:
        with open(image_path, "rb") as f:
            image_data = base64.b64encode(f.read()).decode("utf-8")
    except Exception as e:
        reason = f"FileReadError: {e}"
        fail_record = {
            "model": "Claude",
            "image_path": image_path,
            "reason": reason,
            "raw_content": "",
        }
        return None, fail_record

    # 2) Guess correct media type (image/jpeg or image/png, etc.)
    media_type = guess_media_type(image_path)

    # 3) Build user message with the new synthetic evaluation prompt
    user_message = [
        {
            "type": "image",
            "source": {
                "type": "base64",
                "media_type": media_type,  # use the guessed type
                "data": image_data,
            },
        },
        {
            "type": "text",
            "text": SYNTHETIC_EVALUATION_PROMPT,
        },
    ]

    client = anthropic.Anthropic(api_key=ANTHROPIC_KEY)
    try:
        response = client.messages.create(
            model=ANTHROPIC_MODEL,
            messages=[{"role": "user", "content": user_message}],
            max_tokens=1024,  # Increased for analysis section
            temperature=0.0,
        )
    except Exception as e:
        # Capture full error in raw_content for debugging
        fail_record = {
            "model": "Claude",
            "image_path": image_path,
            "reason": f"APICallError: {str(e)}",
            "raw_content": f"{str(e)}",
        }
        return None, fail_record

    if not response.content or len(response.content) == 0:
        fail_record = {
            "model": "Claude",
            "image_path": image_path,
            "reason": "EmptyContent",
            "raw_content": "",
        }
        return None, fail_record

    raw_reply = response.content[0].text.strip()
    if not raw_reply:
        fail_record = {
            "model": "Claude",
            "image_path": image_path,
            "reason": "EmptyReply",
            "raw_content": "",
        }
        return None, fail_record

    # Extract JSON from the <labels> section
    labels_pattern = r"<labels>\s*(\{.*?\})\s*</labels>"
    labels_match = re.search(labels_pattern, raw_reply, re.DOTALL)

    if labels_match:
        json_str = labels_match.group(1)
        try:
            labels_dict = json.loads(json_str)
            return labels_dict, None
        except json.JSONDecodeError:
            fail_record = {
                "model": "Claude",
                "image_path": image_path,
                "reason": "JSONParseError",
                "raw_content": raw_reply[:300],
            }
            return None, fail_record
    else:
        # If no <labels> tags found, try our robust extractor
        from src.labelling.llm_calls import extract_json_from_code_block

        cleaned = extract_json_from_code_block(raw_reply)
        try:
            labels_dict = json.loads(cleaned)
            return labels_dict, None
        except json.JSONDecodeError:
            fail_record = {
                "model": "Claude",
                "image_path": image_path,
                "reason": "JSONParseError",
                "raw_content": raw_reply[:300],
            }
            return None, fail_record


def evaluate_final_synthetic_images(
    final_csv_path: str,
    diffusion_model: str = "bfai",
    output_dir: str = ".",
    sample_size: int = None,
    proportion_real: float = None,
    llm_types: list = ["claude", "gpt"],
    head_only: int = None,
) -> tuple:
    """
    Specialized function for evaluating the final set of synthetic images.
    This function handles the specific format of the final output CSV.

    Parameters
    ----------
    final_csv_path : str
        Path to the CSV file containing the final synthetic image information.
    diffusion_model : str
        The diffusion model used to generate the images (e.g., "bfai", "sd_16").
    output_dir : str
        The folder where result CSVs and metadata should be saved.
    sample_size : int, optional
        If given, randomly sample this many rows (useful for quick tests).
    proportion_real : float, optional
        If provided with sample_size, controls the proportion of real images (0.0-1.0).
        Default (None) keeps the original distribution.
    llm_types : list
        A list of LLM identifiers to call for classification, e.g. ["claude", "gpt"].
    head_only : int, optional
        If set, only process the first 'head_only' rows (useful for debugging).

    Returns
    -------
    (str, str)
        A tuple of (results_path, failure_stats_path) to the final CSV files generated.
    """
    import os
    import sys
    import pandas as pd
    import numpy as np
    from tqdm import tqdm
    import json
    import time
    from collections import defaultdict

    # Determine naming prefix depending on test mode vs full mode
    if head_only:
        print(f"TEST MODE: only first {head_only} rows of the CSV.")
        prefix = "final_test_evaluation"
    else:
        prefix = "final_evaluation"

    os.makedirs(output_dir, exist_ok=True)

    # Load and process the CSV
    print(f"Loading data from: {final_csv_path}")

    # Try to open the file with various delimiters
    try:
        # First attempt with inferred delimiter
        df = pd.read_csv(final_csv_path, sep=None, engine="python")
        print(f"Successfully loaded file with auto-detected delimiter: {len(df)} rows")
    except Exception as e:
        print(f"Error with auto-detection: {e}")
        try:
            # Try with tab delimiter and error handling
            df = pd.read_csv(final_csv_path, sep="\t", on_bad_lines="warn")
            print(
                f"Loaded tab-delimited file with {len(df)} rows (some lines may have been skipped)"
            )
        except Exception as e2:
            print(f"Error parsing as tab-delimited: {e2}")
            try:
                # Try with comma delimiter and error handling
                df = pd.read_csv(final_csv_path, on_bad_lines="warn")
                print(
                    f"Loaded comma-delimited file with {len(df)} rows (some lines may have been skipped)"
                )
            except Exception as e3:
                print(f"All parsing methods failed: {e3}")
                return None, None

    # Optionally limit row count with head_only
    if head_only and head_only < len(df):
        df = df.head(head_only)

    # Handle sampling with proportion control
    if sample_size and sample_size < len(df):
        if proportion_real is not None and "source_type" in df.columns:
            # Split data by source type
            real_df = df[df["source_type"] == "real"]
            hypo_df = df[df["source_type"] == "hypothetical"]
            other_df = df[~df["source_type"].isin(["real", "hypothetical"])]

            # Calculate number of samples for each category
            real_samples = int(sample_size * proportion_real)
            hypo_samples = sample_size - real_samples

            # Check if we have enough samples in each category
            if len(real_df) < real_samples:
                print(
                    f"Warning: Not enough real images ({len(real_df)}) to sample {real_samples}."
                )
                real_samples = len(real_df)
                hypo_samples = sample_size - real_samples

            if len(hypo_df) < hypo_samples:
                print(
                    f"Warning: Not enough hypothetical images ({len(hypo_df)}) to sample {hypo_samples}."
                )
                hypo_samples = len(hypo_df)
                # If we also don't have enough real samples, we'll end up with fewer than sample_size

            # Sample from each category
            if real_samples > 0:
                sampled_real = real_df.sample(real_samples, random_state=42)
            else:
                sampled_real = pd.DataFrame(columns=real_df.columns)

            if hypo_samples > 0:
                sampled_hypo = hypo_df.sample(hypo_samples, random_state=42)
            else:
                sampled_hypo = pd.DataFrame(columns=hypo_df.columns)

            # Combine the samples
            df = pd.concat([sampled_real, sampled_hypo, other_df])

            # Print out the distribution
            real_count = len(df[df["source_type"] == "real"])
            hypo_count = len(df[df["source_type"] == "hypothetical"])
            total = real_count + hypo_count

            print(f"Sampled {total} images:")
            print(f"  Real: {real_count} ({real_count/total*100:.1f}%)")
            print(f"  Hypothetical: {hypo_count} ({hypo_count/total*100:.1f}%)")
        else:
            # Regular random sampling
            df = df.sample(sample_size, random_state=42)
            if "source_type" in df.columns:
                real_count = len(df[df["source_type"] == "real"])
                hypo_count = len(df[df["source_type"] == "hypothetical"])
                total = real_count + hypo_count
                print(f"Sampled {total} images (original distribution):")
                print(f"  Real: {real_count} ({real_count/total*100:.1f}%)")
                print(f"  Hypothetical: {hypo_count} ({hypo_count/total*100:.1f}%)")

    # Ensure required columns exist, create them if missing
    required = [
        "damage_severity",
        "informative",
        "humanitarian",
        "disaster_types",
        "aug_img_path",
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        print(f"Warning: missing columns: {missing}")
        for col in missing:
            df[col] = None

    # Process the rows
    all_rows = []
    fail_stats = {
        "diffusion_model": [],
        "source_type": [],
        "prompt_key": [],
        "llm_model": [],
        "total_images": [],
        "missing_images": [],
        "failure_rate": [],
    }

    # Tracking structures for failures
    total_images = 0
    missing_images = 0
    src_tot = defaultdict(int)
    src_fail = defaultdict(int)
    llm_tot = defaultdict(int)
    llm_fail = defaultdict(int)

    # Process each row
    for _, row in tqdm(
        df.iterrows(), total=len(df), desc=f"Evaluating {diffusion_model}"
    ):
        # Get the image path
        image_path = row.get("aug_img_path", None)
        source_type = row.get("source_type", "unknown")

        # Extract prompt key
        prompt_key = "unknown"

        # Get LLM model name if available
        llm_model = row.get("model_name", "unknown")

        # Increment counters
        total_images += 1
        src_tot[source_type] += 1
        llm_tot[llm_model] += 1

        # Check if image exists
        if (
            image_path is None
            or not isinstance(image_path, str)
            or not os.path.exists(image_path)
        ):
            missing_images += 1
            src_fail[source_type] += 1
            llm_fail[llm_model] += 1

            rdict = {
                "diffusion_model": diffusion_model,
                "source_type": source_type,
                "prompt_key": prompt_key,
                "llm_model": llm_model,
                "image_path": image_path,
                "image_missing": True,
            }

            # Keep original labels for reference
            for t in [
                "damage_severity",
                "informative",
                "humanitarian",
                "disaster_types",
            ]:
                rdict[f"original_{t}"] = row.get(t, None)

            # LLM predictions are None if missing
            for llm in llm_types:
                for t in [
                    "damage_severity",
                    "informative",
                    "humanitarian",
                    "disaster_types",
                ]:
                    rdict[f"{llm}_{t}"] = None

            all_rows.append(rdict)
            continue

        # Image exists, proceed with LLM evaluation
        rdict = {
            "diffusion_model": diffusion_model,
            "source_type": source_type,
            "prompt_key": prompt_key,
            "llm_model": llm_model,
            "image_path": image_path,
            "image_missing": False,
        }

        # Store original labels
        for t in ["damage_severity", "informative", "humanitarian", "disaster_types"]:
            rdict[f"original_{t}"] = row.get(t, None)

        # Import LLM API functions
        from src.labelling.llm_calls import call_gpt_api
        from src.augmentation.synthetic_tests import call_claude_api_synthetic

        # Call Claude if requested
        if "claude" in llm_types:
            try:
                claude_labels, claude_fail = call_claude_api_synthetic(image_path)
            except Exception as e:
                claude_labels = None
                claude_fail = {"reason": f"Exception: {str(e)}"}

            # Process results
            if claude_fail:
                # If fail, set predicted to None
                for t in [
                    "damage_severity",
                    "informative",
                    "humanitarian",
                    "disaster_types",
                ]:
                    rdict[f"claude_{t}"] = None
                rdict["claude_fail_reason"] = claude_fail.get("reason", "")
            else:
                for t in [
                    "damage_severity",
                    "informative",
                    "humanitarian",
                    "disaster_types",
                ]:
                    rdict[f"claude_{t}"] = (
                        claude_labels.get(t, None) if claude_labels else None
                    )

        # Call GPT if requested
        if "gpt" in llm_types:
            gpt_labels, gpt_fail = call_gpt_api(image_path)

            if gpt_fail:
                for t in [
                    "damage_severity",
                    "informative",
                    "humanitarian",
                    "disaster_types",
                ]:
                    rdict[f"gpt_{t}"] = None
                rdict["gpt_fail_reason"] = gpt_fail.get("reason", "")
            else:
                for t in [
                    "damage_severity",
                    "informative",
                    "humanitarian",
                    "disaster_types",
                ]:
                    rdict[f"gpt_{t}"] = gpt_labels.get(t, None) if gpt_labels else None

        all_rows.append(rdict)

    # Create final DataFrames
    results_df = pd.DataFrame(all_rows)

    # Aggregate failure stats
    fail_stats["diffusion_model"].append(diffusion_model)
    fail_stats["prompt_key"].append("all")
    fail_stats["source_type"].append("all")
    fail_stats["llm_model"].append("all")
    fail_stats["total_images"].append(total_images)
    fail_stats["missing_images"].append(missing_images)
    fail_stats["failure_rate"].append(
        100 * missing_images / total_images if total_images > 0 else 0
    )

    # Add source_type stats
    for s_type, tot in src_tot.items():
        fails = src_fail[s_type]
        fail_stats["diffusion_model"].append(diffusion_model)
        fail_stats["prompt_key"].append("all")
        fail_stats["source_type"].append(s_type)
        fail_stats["llm_model"].append("all")
        fail_stats["total_images"].append(tot)
        fail_stats["missing_images"].append(fails)
        fail_stats["failure_rate"].append(100 * fails / tot if tot > 0 else 0)

    # Add llm_model stats
    for l_model, tot in llm_tot.items():
        fails = llm_fail[l_model]
        fail_stats["diffusion_model"].append(diffusion_model)
        fail_stats["prompt_key"].append("all")
        fail_stats["source_type"].append("all")
        fail_stats["llm_model"].append(l_model)
        fail_stats["total_images"].append(tot)
        fail_stats["missing_images"].append(fails)
        fail_stats["failure_rate"].append(100 * fails / tot if tot > 0 else 0)

    fail_df = pd.DataFrame(fail_stats)

    # Calculate match columns for comparison
    for llm in llm_types:
        for t in ["damage_severity", "informative", "humanitarian", "disaster_types"]:
            match_col = f"{llm}_{t}_match"
            pred_col = f"{llm}_{t}"
            orig_col = f"original_{t}"

            # Create a new column with explicit dtype to avoid warnings
            results_df[match_col] = pd.Series(
                np.nan, index=results_df.index, dtype="float64"
            )

            # Create the mask for valid entries
            valid_mask = (
                ~results_df["image_missing"]
                & results_df[pred_col].notna()
                & results_df[orig_col].notna()
            )

            # Convert boolean comparison to float64 before assignment
            match_values = (
                results_df.loc[valid_mask, pred_col]
                == results_df.loc[valid_mask, orig_col]
            ).astype("float64")

            # Assign values to the matched rows
            results_df.loc[valid_mask, match_col] = match_values

    # Save results
    res_path = os.path.join(output_dir, f"{prefix}_results.csv")
    fail_path = os.path.join(output_dir, f"{prefix}_failure_stats.csv")
    results_df.to_csv(res_path, index=False)
    fail_df.to_csv(fail_path, index=False)
    print(f"Saved results: {res_path}")
    print(f"Saved failure stats: {fail_path}")

    # Save metadata JSON
    meta = {
        "timestamp": str(pd.Timestamp.now()),
        "diffusion_model": diffusion_model,
        "llm_types": llm_types,
        "sample_size": sample_size,
        "proportion_real": proportion_real,
        "head_only": head_only,
        "rows_in_results": len(results_df),
    }
    with open(os.path.join(output_dir, f"{prefix}_metadata.json"), "w") as f:
        json.dump(meta, f, indent=2)

    return res_path, fail_path


def analyze_final_synthetic_results(
    results_path: str,
    failure_stats_path: str = None,
    output_dir: str = None,
) -> dict:
    """
    Analyze the results from evaluate_final_synthetic_images function.
    Shows detailed breakdown by class for each task and separates results by source type.

    Parameters
    ----------
    results_path : str
        Path to the CSV file containing the evaluation results.
    failure_stats_path : str, optional
        Path to the CSV file containing failure statistics.
    output_dir : str, optional
        Directory to save additional output files (currently unused).

    Returns
    -------
    dict
        Dictionary containing analysis metrics.
    """
    import pandas as pd
    import numpy as np
    from IPython.display import display, HTML
    import os

    print(f"Analyzing results from: {results_path}")

    # Load the results
    if results_path.endswith(".pkl"):
        df = pd.read_pickle(results_path)
    else:
        df = pd.read_csv(results_path)

    print(f"Loaded results with {len(df)} rows")

    # Filter out missing images
    df = df[~df["image_missing"]].copy()
    print(f"Non-missing images: {len(df)}")

    # Define tasks and source types
    tasks = ["damage_severity", "informative", "humanitarian", "disaster_types"]
    source_types = ["real", "hypothetical"]

    # Get all unique values for each task
    class_values = {}
    for task in tasks:
        col = f"original_{task}"
        if col in df.columns:
            unique_vals = df[col].dropna().unique()
            class_values[task] = sorted(list(unique_vals))

    # Calculate match rates for each task, class, and source type
    llm_results = {}
    for llm in ["claude", "gpt"]:
        task_results = {}

        # Process each task
        for task in tasks:
            match_col = f"{llm}_{task}_match"

            # Skip if match column doesn't exist
            if match_col not in df.columns:
                continue

            # Track results by source type and overall
            source_type_results = {}
            class_results = {}

            # Calculate overall task metrics (regardless of source type)
            valid_mask = df[match_col].notna()
            valid_count = valid_mask.sum()

            if valid_count > 0:
                task_match_rate = 100 * df.loc[valid_mask, match_col].mean()
            else:
                task_match_rate = np.nan

            # Calculate metrics for each source type
            for source_type in source_types:
                # Filter by source type
                source_df = df[df["source_type"] == source_type]
                source_valid_mask = source_df[match_col].notna()
                source_valid_count = source_valid_mask.sum()

                if source_valid_count > 0:
                    source_match_rate = (
                        100 * source_df.loc[source_valid_mask, match_col].mean()
                    )
                    source_type_results[source_type] = {
                        "match_rate": source_match_rate,
                        "count": source_valid_count,
                    }
                else:
                    source_type_results[source_type] = {
                        "match_rate": np.nan,
                        "count": 0,
                    }

            # Calculate per-class metrics
            if task in class_values:
                for class_val in class_values[task]:
                    class_results_by_source = {}

                    # Overall metrics for this class
                    class_mask = (df[f"original_{task}"] == class_val) & valid_mask
                    class_count = class_mask.sum()

                    if class_count > 0:
                        class_match_rate = 100 * df.loc[class_mask, match_col].mean()
                    else:
                        class_match_rate = np.nan

                    # Metrics by source type for this class
                    for source_type in source_types:
                        source_df = df[df["source_type"] == source_type]
                        source_class_mask = (
                            source_df[f"original_{task}"] == class_val
                        ) & source_df[match_col].notna()
                        source_class_count = source_class_mask.sum()

                        if source_class_count > 0:
                            source_class_match_rate = (
                                100 * source_df.loc[source_class_mask, match_col].mean()
                            )
                            class_results_by_source[source_type] = {
                                "match_rate": source_class_match_rate,
                                "count": source_class_count,
                            }
                        else:
                            class_results_by_source[source_type] = {
                                "match_rate": np.nan,
                                "count": 0,
                            }

                    # Store the class results
                    class_results[class_val] = {
                        "overall": {
                            "match_rate": class_match_rate,
                            "count": class_count,
                        },
                        "by_source": class_results_by_source,
                    }

            # Store results for this task
            task_results[task] = {
                "overall": {"match_rate": task_match_rate, "count": valid_count},
                "by_source": source_type_results,
                "classes": class_results,
            }

        llm_results[llm] = task_results

    # Create a formatted HTML table for display
    html = """
    <style>
        .results-table {
            border-collapse: collapse;
            width: 100%;
            margin-bottom: 20px;
            font-family: Arial, sans-serif;
            font-size: 14px;
        }
        .results-table th, .results-table td {
            border: 1px solid #ddd;
            padding: 6px;
            text-align: center;
        }
        .results-table th {
            background-color: #333;
            color: white;
            font-weight: bold;
        }
        .results-table tr.task-row {
            background-color: #222;
            color: white;
            font-weight: bold;
        }
        .results-table tr.class-row {
            background-color: #333;
            color: white;
        }
        .results-table tr:nth-child(even):not(.task-row) {
            background-color: #2e2e2e;
        }
        .results-table .source-header {
            border-bottom: none;
        }
        .results-table .model-header {
            border-bottom: 1px solid #ddd;
        }
    </style>
    <h3>Synthetic Image Evaluation Results</h3>
    <table class="results-table">
        <tr>
            <th rowspan="2" style="text-align:left; width:20%;">Task/Class</th>
            <th colspan="3" class="source-header">Claude</th>
            <th colspan="3" class="source-header">GPT</th>
            <th colspan="3" class="source-header">Average</th>
        </tr>
        <tr>
            <th class="model-header">Real</th>
            <th class="model-header">Hypo</th>
            <th class="model-header">All</th>
            <th class="model-header">Real</th>
            <th class="model-header">Hypo</th>
            <th class="model-header">All</th>
            <th class="model-header">Real</th>
            <th class="model-header">Hypo</th>
            <th class="model-header">All</th>
        </tr>
    """

    # Helper function to format values
    def format_value(val):
        if pd.isna(val):
            return "N/A"
        return f"{val:.1f}%"

    # Add rows for each task and its classes
    for task in tasks:
        # Task row (bold)
        task_name = task.replace("_", " ").title()

        # Get match rates for this task
        claude_real = (
            llm_results.get("claude", {})
            .get(task, {})
            .get("by_source", {})
            .get("real", {})
            .get("match_rate", np.nan)
        )
        claude_hypo = (
            llm_results.get("claude", {})
            .get(task, {})
            .get("by_source", {})
            .get("hypothetical", {})
            .get("match_rate", np.nan)
        )
        claude_all = (
            llm_results.get("claude", {})
            .get(task, {})
            .get("overall", {})
            .get("match_rate", np.nan)
        )

        gpt_real = (
            llm_results.get("gpt", {})
            .get(task, {})
            .get("by_source", {})
            .get("real", {})
            .get("match_rate", np.nan)
        )
        gpt_hypo = (
            llm_results.get("gpt", {})
            .get(task, {})
            .get("by_source", {})
            .get("hypothetical", {})
            .get("match_rate", np.nan)
        )
        gpt_all = (
            llm_results.get("gpt", {})
            .get(task, {})
            .get("overall", {})
            .get("match_rate", np.nan)
        )

        # Calculate averages
        avg_real = (
            np.nanmean([claude_real, gpt_real])
            if not (pd.isna(claude_real) and pd.isna(gpt_real))
            else np.nan
        )
        avg_hypo = (
            np.nanmean([claude_hypo, gpt_hypo])
            if not (pd.isna(claude_hypo) and pd.isna(gpt_hypo))
            else np.nan
        )
        avg_all = (
            np.nanmean([claude_all, gpt_all])
            if not (pd.isna(claude_all) and pd.isna(gpt_all))
            else np.nan
        )

        html += f"""
        <tr class="task-row">
            <td style="text-align:left;">{task_name}</td>
            <td>{format_value(claude_real)}</td>
            <td>{format_value(claude_hypo)}</td>
            <td>{format_value(claude_all)}</td>
            <td>{format_value(gpt_real)}</td>
            <td>{format_value(gpt_hypo)}</td>
            <td>{format_value(gpt_all)}</td>
            <td>{format_value(avg_real)}</td>
            <td>{format_value(avg_hypo)}</td>
            <td>{format_value(avg_all)}</td>
        </tr>
        """

        # Add class rows under each task
        if task in class_values:
            for class_val in class_values[task]:
                class_name = class_val.replace("_", " ").title()

                # Get match rates for this class
                claude_class_real = (
                    llm_results.get("claude", {})
                    .get(task, {})
                    .get("classes", {})
                    .get(class_val, {})
                    .get("by_source", {})
                    .get("real", {})
                    .get("match_rate", np.nan)
                )
                claude_class_hypo = (
                    llm_results.get("claude", {})
                    .get(task, {})
                    .get("classes", {})
                    .get(class_val, {})
                    .get("by_source", {})
                    .get("hypothetical", {})
                    .get("match_rate", np.nan)
                )
                claude_class_all = (
                    llm_results.get("claude", {})
                    .get(task, {})
                    .get("classes", {})
                    .get(class_val, {})
                    .get("overall", {})
                    .get("match_rate", np.nan)
                )

                gpt_class_real = (
                    llm_results.get("gpt", {})
                    .get(task, {})
                    .get("classes", {})
                    .get(class_val, {})
                    .get("by_source", {})
                    .get("real", {})
                    .get("match_rate", np.nan)
                )
                gpt_class_hypo = (
                    llm_results.get("gpt", {})
                    .get(task, {})
                    .get("classes", {})
                    .get(class_val, {})
                    .get("by_source", {})
                    .get("hypothetical", {})
                    .get("match_rate", np.nan)
                )
                gpt_class_all = (
                    llm_results.get("gpt", {})
                    .get(task, {})
                    .get("classes", {})
                    .get(class_val, {})
                    .get("overall", {})
                    .get("match_rate", np.nan)
                )

                # Calculate averages
                avg_class_real = (
                    np.nanmean([claude_class_real, gpt_class_real])
                    if not (pd.isna(claude_class_real) and pd.isna(gpt_class_real))
                    else np.nan
                )
                avg_class_hypo = (
                    np.nanmean([claude_class_hypo, gpt_class_hypo])
                    if not (pd.isna(claude_class_hypo) and pd.isna(gpt_class_hypo))
                    else np.nan
                )
                avg_class_all = (
                    np.nanmean([claude_class_all, gpt_class_all])
                    if not (pd.isna(claude_class_all) and pd.isna(gpt_class_all))
                    else np.nan
                )

                html += f"""
                <tr class="class-row">
                    <td style="text-align:left; padding-left:20px;">{class_name}</td>
                    <td>{format_value(claude_class_real)}</td>
                    <td>{format_value(claude_class_hypo)}</td>
                    <td>{format_value(claude_class_all)}</td>
                    <td>{format_value(gpt_class_real)}</td>
                    <td>{format_value(gpt_class_hypo)}</td>
                    <td>{format_value(gpt_class_all)}</td>
                    <td>{format_value(avg_class_real)}</td>
                    <td>{format_value(avg_class_hypo)}</td>
                    <td>{format_value(avg_class_all)}</td>
                </tr>
                """

    # Calculate overall averages across all tasks
    overall_claude_real = np.nanmean(
        [
            llm_results.get("claude", {})
            .get(task, {})
            .get("by_source", {})
            .get("real", {})
            .get("match_rate", np.nan)
            for task in tasks
        ]
    )
    overall_claude_hypo = np.nanmean(
        [
            llm_results.get("claude", {})
            .get(task, {})
            .get("by_source", {})
            .get("hypothetical", {})
            .get("match_rate", np.nan)
            for task in tasks
        ]
    )
    overall_claude_all = np.nanmean(
        [
            llm_results.get("claude", {})
            .get(task, {})
            .get("overall", {})
            .get("match_rate", np.nan)
            for task in tasks
        ]
    )

    overall_gpt_real = np.nanmean(
        [
            llm_results.get("gpt", {})
            .get(task, {})
            .get("by_source", {})
            .get("real", {})
            .get("match_rate", np.nan)
            for task in tasks
        ]
    )
    overall_gpt_hypo = np.nanmean(
        [
            llm_results.get("gpt", {})
            .get(task, {})
            .get("by_source", {})
            .get("hypothetical", {})
            .get("match_rate", np.nan)
            for task in tasks
        ]
    )
    overall_gpt_all = np.nanmean(
        [
            llm_results.get("gpt", {})
            .get(task, {})
            .get("overall", {})
            .get("match_rate", np.nan)
            for task in tasks
        ]
    )

    overall_avg_real = np.nanmean([overall_claude_real, overall_gpt_real])
    overall_avg_hypo = np.nanmean([overall_claude_hypo, overall_gpt_hypo])
    overall_avg_all = np.nanmean([overall_claude_all, overall_gpt_all])

    # Add overall row
    html += f"""
    <tr class="task-row">
        <td style="text-align:left;">OVERALL</td>
        <td>{format_value(overall_claude_real)}</td>
        <td>{format_value(overall_claude_hypo)}</td>
        <td>{format_value(overall_claude_all)}</td>
        <td>{format_value(overall_gpt_real)}</td>
        <td>{format_value(overall_gpt_hypo)}</td>
        <td>{format_value(overall_gpt_all)}</td>
        <td>{format_value(overall_avg_real)}</td>
        <td>{format_value(overall_avg_hypo)}</td>
        <td>{format_value(overall_avg_all)}</td>
    </tr>
    """

    html += "</table>"

    # Display the HTML table
    display(HTML(html))

    # Print source type distribution
    source_counts = df["source_type"].value_counts()
    print("\nSource Type Distribution:")
    for source, count in source_counts.items():
        print(f"  {source}: {count} images ({count/len(df)*100:.1f}%)")

    # Count error types if present
    if "claude_fail_reason" in df.columns or "gpt_fail_reason" in df.columns:
        print("\nError Counts:")

        if "claude_fail_reason" in df.columns:
            claude_errors = df["claude_fail_reason"].dropna().value_counts()
            if not claude_errors.empty:
                print("  Claude Errors:")
                for error, count in claude_errors.items():
                    print(f"    {error}: {count}")

        if "gpt_fail_reason" in df.columns:
            gpt_errors = df["gpt_fail_reason"].dropna().value_counts()
            if not gpt_errors.empty:
                print("  GPT Errors:")
                for error, count in gpt_errors.items():
                    print(f"    {error}: {count}")

    return None


def synthetic_image_samples(
    results_csv: str, task_name: str = None, class_name: str = None, seed: int = 42
):
    """
    Display visualizations showing correctly and incorrectly classified synthetic images
    directly in the notebook.

    For each class, displays 8 examples in a 2x4 grid:
    - Top row: Correctly and incorrectly classified real images
    - Bottom row: Correctly and incorrectly classified hypothetical images

    Parameters
    ----------
    results_csv : str
        Path to the CSV file containing synthetic image evaluation results
    task_name : str, optional
        Name of the task to analyze (damage_severity, informative, humanitarian, disaster_types)
        If None, all tasks will be processed
    class_name : str, optional
        The specific class name to analyze. If None, all classes in the task will be processed
    seed : int
        Random seed for reproducible sampling
    """
    import os
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from tqdm import tqdm
    from IPython.display import display

    # Set random seed for reproducibility
    np.random.seed(seed)

    # Define the tasks (same as in the original code)
    TASKS = ["damage_severity", "informative", "humanitarian", "disaster_types"]

    # Load results dataframe
    print(f"Loading data from {results_csv}")
    df = pd.read_csv(results_csv)

    # Filter out missing images
    df = df[~df["image_missing"]]
    print(f"Found {len(df)} valid images after filtering out missing ones")

    # Convert match columns to boolean if needed
    for col in df.columns:
        if "_match" in col:
            # Handle 0/1, True/False, etc.
            if df[col].dtype != bool:
                df[col] = df[col].astype(float).astype(bool)

    # Determine which tasks to process
    if task_name is not None:
        if task_name in TASKS:
            tasks_to_process = [task_name]
        else:
            raise ValueError(f"Invalid task name: {task_name}. Must be one of {TASKS}")
    else:
        tasks_to_process = TASKS

    # Process each task
    for task in tasks_to_process:
        # Get unique classes for this task
        classes = df[f"original_{task}"].dropna().unique()

        # If a specific class is requested, only process that one
        if class_name is not None:
            if class_name in classes:
                classes = [class_name]
            else:
                print(
                    f"Warning: Class '{class_name}' not found in task '{task}'. Skipping."
                )
                continue

        print(f"Processing {len(classes)} classes for task '{task}'")

        # Process each class
        for cls in classes:
            # Skip if class is NaN
            if pd.isna(cls):
                continue

            print(f"\nProcessing {task} - {cls}")

            # Filter to the specific class
            class_df = df[df[f"original_{task}"] == cls]

            if len(class_df) == 0:
                print(f"No examples found for class '{cls}' in task '{task}'")
                continue

            # Define conditions for correct and incorrect classifications
            # Both Claude and GPT must agree for correct/incorrect categories
            correct_mask = (class_df[f"claude_{task}_match"] == True) & (
                class_df[f"gpt_{task}_match"] == True
            )

            incorrect_mask = (class_df[f"claude_{task}_match"] == False) & (
                class_df[f"gpt_{task}_match"] == False
            )

            # Split by source type (real vs hypothetical)
            # Real images are explicitly labeled as 'real'
            real_mask = class_df["source_type"] == "real"
            hypo_mask = (
                ~real_mask
            )  # Anything not explicitly 'real' is treated as hypothetical

            # Get the four categories of images
            correct_real = class_df[correct_mask & real_mask]
            correct_hypo = class_df[correct_mask & hypo_mask]
            incorrect_real = class_df[incorrect_mask & real_mask]
            incorrect_hypo = class_df[incorrect_mask & hypo_mask]

            print(
                f"  Found {len(correct_real)} correct real, {len(correct_hypo)} correct hypothetical"
            )
            print(
                f"  Found {len(incorrect_real)} incorrect real, {len(incorrect_hypo)} incorrect hypothetical"
            )

            # Function to sample up to n rows, or return all if fewer than n exist
            def sample_rows(df, n=2):
                if len(df) == 0:
                    return df
                if len(df) <= n:
                    return df
                return df.sample(n)

            # Sample 2 examples from each category
            correct_real_samples = sample_rows(correct_real)
            correct_hypo_samples = sample_rows(correct_hypo)
            incorrect_real_samples = sample_rows(incorrect_real)
            incorrect_hypo_samples = sample_rows(incorrect_hypo)

            # Create figure for visualization
            fig, axes = plt.subplots(2, 4, figsize=(20, 10))

            # Add main title
            fig.suptitle(f"{task} - {cls}", fontsize=16)

            # Set titles for columns
            axes[0, 0].set_title("Correct - Real", fontsize=12)
            axes[0, 1].set_title("Correct - Real", fontsize=12)
            axes[0, 2].set_title("Incorrect - Real", fontsize=12)
            axes[0, 3].set_title("Incorrect - Real", fontsize=12)
            axes[1, 0].set_title("Correct - Hypothetical", fontsize=12)
            axes[1, 1].set_title("Correct - Hypothetical", fontsize=12)
            axes[1, 2].set_title("Incorrect - Hypothetical", fontsize=12)
            axes[1, 3].set_title("Incorrect - Hypothetical", fontsize=12)

            # Function to plot an image if available
            def plot_image(ax, row=None):
                if row is not None:
                    try:
                        img = plt.imread(row["image_path"])
                        ax.imshow(img)

                        # Extract just the filename from the path
                        filename = os.path.basename(row["image_path"])

                        # Add caption with filename, original and predicted labels
                        caption = f"File: {filename}\n"
                        caption += f"Original: {row[f'original_{task}']}\n"
                        caption += f"Claude: {row[f'claude_{task}']}\n"
                        caption += f"GPT: {row[f'gpt_{task}']}"
                        ax.set_xlabel(caption, fontsize=8)
                    except Exception as e:
                        ax.text(
                            0.5,
                            0.5,
                            f"Error loading image: {str(e)[:50]}...",
                            ha="center",
                            va="center",
                            wrap=True,
                        )
                else:
                    ax.text(0.5, 0.5, "No sample available", ha="center", va="center")

                ax.set_xticks([])
                ax.set_yticks([])

            # Plot correct real samples (first two columns, top row)
            for i, (_, row) in enumerate(correct_real_samples.iterrows()):
                if i < 2:
                    plot_image(axes[0, i], row)

            # Fill any empty slots
            for i in range(len(correct_real_samples), 2):
                plot_image(axes[0, i])

            # Plot incorrect real samples (last two columns, top row)
            for i, (_, row) in enumerate(incorrect_real_samples.iterrows()):
                if i < 2:
                    plot_image(axes[0, i + 2], row)

            # Fill any empty slots
            for i in range(len(incorrect_real_samples), 2):
                plot_image(axes[0, i + 2])

            # Plot correct hypothetical samples (first two columns, bottom row)
            for i, (_, row) in enumerate(correct_hypo_samples.iterrows()):
                if i < 2:
                    plot_image(axes[1, i], row)

            # Fill any empty slots
            for i in range(len(correct_hypo_samples), 2):
                plot_image(axes[1, i])

            # Plot incorrect hypothetical samples (last two columns, bottom row)
            for i, (_, row) in enumerate(incorrect_hypo_samples.iterrows()):
                if i < 2:
                    plot_image(axes[1, i + 2], row)

            # Fill any empty slots
            for i in range(len(incorrect_hypo_samples), 2):
                plot_image(axes[1, i + 2])

            plt.tight_layout(
                rect=[0, 0, 1, 0.95]
            )  # Adjust layout to make room for suptitle

            # Display the figure in the notebook
            display(fig)
            plt.close(fig)


def plot_final_confusion_matrices(csv: str, show_metrics: bool = True) -> None:
    """
    Load the final test CSV and produce confusion matrices for both claude and gpt
    predictions for the single model.

    Parameters
    ----------
    csv : str
        File path to the CSV containing columns for original labels, claude and gpt predictions.
    show_metrics : bool, optional
        Whether to display accuracy metrics from the match columns, default True.

    Returns
    -------
    None
        This function prints the confusion matrix plots directly.
    """
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import confusion_matrix
    from config import LABEL_MAPPINGS

    # -------------------------------------------------------------------------
    # 1) Load the CSV into a DataFrame
    # -------------------------------------------------------------------------
    df = pd.read_csv(csv)
    print(f"Loaded {len(df)} rows from {csv}")

    if df.empty:
        print("[Warning] The DataFrame is empty. No plots to show.")
        return

    # -------------------------------------------------------------------------
    # 2) We will produce confusion matrices for these four tasks
    # -------------------------------------------------------------------------
    tasks = ["disaster_types", "informative", "humanitarian", "damage_severity"]

    # Titles to display on the subplots
    task_titles = {
        "disaster_types": "Disaster Types",
        "informative": "Informative",
        "humanitarian": "Humanitarian",
        "damage_severity": "Damage Severity",
    }

    # -------------------------------------------------------------------------
    # 3) Shorter label names for visualization
    # -------------------------------------------------------------------------
    # Define shorter display names for each label
    LABEL_DISPLAY_NAMES = {
        "damage_severity": {
            "little_or_none": "none",
            "mild": "mild",
            "severe": "severe",
        },
        "informative": {"not_informative": "not inf", "informative": "inf"},
        "humanitarian": {
            "not_humanitarian": "not hum",
            "affected_injured_or_dead_people": "injured",
            "infrastructure_and_utility_damage": "infra",
            "rescue_volunteering_or_donation_effort": "rescue",
        },
        "disaster_types": {
            "not_disaster": "none",
            "earthquake": "quake",
            "fire": "fire",
            "flood": "flood",
            "hurricane": "hurr.",
            "landslide": "land.",
            "other_disaster": "other",
        },
    }

    # -------------------------------------------------------------------------
    # 4) Define remapping for non-standard values
    # -------------------------------------------------------------------------
    NON_STANDARD_MAPPINGS = {
        "damage_severity": {
            "not_disaster": "little_or_none",
            "not_informative": "little_or_none",
            "not_applicable": "little_or_none",
        },
        "disaster_types": {"tornado": "other_disaster", "avalanche": "other_disaster"},
    }

    # -------------------------------------------------------------------------
    # 5) Set up the figure grid:
    #    - One row per model (claude/gpt)
    #    - Four columns (one for each task)
    # -------------------------------------------------------------------------
    models = ["claude", "gpt"]
    n_rows = len(models)
    n_cols = len(tasks)

    fig, axes = plt.subplots(
        n_rows, n_cols, figsize=(5.5 * n_cols, 4.5 * n_rows), squeeze=False
    )

    # -------------------------------------------------------------------------
    # 6) Add overall figure title showing the model name if available
    # -------------------------------------------------------------------------
    if "llm_model" in df.columns and not df["llm_model"].isna().all():
        model_name = df["llm_model"].iloc[0]
        fig.suptitle(f"Evaluation Results for {model_name}", fontsize=16, y=0.98)

    # -------------------------------------------------------------------------
    # 7) For each model (claude/gpt), produce the 4 confusion matrices
    # -------------------------------------------------------------------------
    for row_idx, model in enumerate(models):
        # Get nice display name for the model
        if model == "claude":
            model_display = "Claude"
        elif model == "gpt":
            model_display = "GPT"
        else:
            model_display = model

        for col_idx, task in enumerate(tasks):
            ax = axes[row_idx, col_idx]

            # Identify the columns for ground-truth and predicted labels
            true_col = f"original_{task}"
            pred_col = f"{model}_{task}"

            # If columns are missing, skip gracefully
            if true_col not in df.columns or pred_col not in df.columns:
                ax.text(
                    0.5, 0.5, "Missing columns", ha="center", va="center", fontsize=12
                )
                ax.set_title(
                    f"{model_display}: {task_titles.get(task, task)} (No Data)"
                )
                ax.axis("off")
                continue

            try:
                # Create a working copy of the dataframe for this task
                task_df = df.copy()

                # Apply non-standard label mapping if there are any for this task
                if task in NON_STANDARD_MAPPINGS:
                    # Map non-standard values in both original and predicted columns
                    for non_standard, standard in NON_STANDARD_MAPPINGS[task].items():
                        # Map in true column
                        mask = task_df[true_col] == non_standard
                        if mask.sum() > 0:
                            task_df.loc[mask, true_col] = standard

                        # Map in predicted column
                        mask = task_df[pred_col] == non_standard
                        if mask.sum() > 0:
                            task_df.loc[mask, pred_col] = standard

                # Get valid labels for this task
                # Handle both formats - if LABEL_MAPPINGS is a dict with integer keys
                if isinstance(LABEL_MAPPINGS[task], dict):
                    # Extract just the values from the dictionary
                    label_list = list(LABEL_MAPPINGS[task].values())
                else:
                    # If it's already a list, use it directly
                    label_list = LABEL_MAPPINGS[task]

                # Filter out rows with NaN values or values not in the valid label list
                mask = task_df[true_col].notna() & task_df[pred_col].notna()
                mask = (
                    mask
                    & task_df[true_col].isin(label_list)
                    & task_df[pred_col].isin(label_list)
                )

                if mask.sum() == 0:
                    ax.text(
                        0.5,
                        0.5,
                        "No valid data points",
                        ha="center",
                        va="center",
                        fontsize=12,
                    )
                    ax.set_title(
                        f"{model_display}: {task_titles.get(task, task)} (No Valid Data)"
                    )
                    ax.axis("off")
                    continue

                # Use only the filtered rows
                y_true = task_df.loc[mask, true_col]
                y_pred = task_df.loc[mask, pred_col]

                # Calculate accuracy if the match column exists and show_metrics is True
                match_col = f"{model}_{task}_match"
                accuracy = None
                if show_metrics and match_col in task_df.columns:
                    # Calculate accuracy from the match column (assuming 1=match, 0=no match)
                    valid_matches = task_df.loc[mask, match_col]
                    if not valid_matches.empty:
                        accuracy = valid_matches.mean() * 100  # Convert to percentage

                # Build confusion matrix with the valid data
                cm = confusion_matrix(y_true, y_pred, labels=label_list)

                # Normalise each row
                cm_sum_per_row = cm.sum(axis=1)[:, np.newaxis]
                cm_normalized = cm.astype("float") / np.clip(cm_sum_per_row, 1e-9, None)

                # Build a custom "positive diagonal, negative off-diagonal" array
                custom_norm = np.zeros_like(cm_normalized)
                for i in range(len(custom_norm)):
                    for j in range(len(custom_norm[i])):
                        if i == j:
                            custom_norm[i, j] = cm_normalized[i, j]
                        else:
                            custom_norm[i, j] = -cm_normalized[i, j]

                # Build the annotation strings
                # E.g. 0.123 -> .12, 1 -> 1.00, 0 -> 0
                annotations = []
                for row in cm_normalized:
                    row_ann = []
                    for x in row:
                        if 0 < x < 1:
                            frac_str = f"{x:.2f}"  # e.g. 0.12
                            if frac_str.startswith("0."):
                                frac_str = frac_str[1:]  # remove leading 0 => .12
                            row_ann.append(frac_str)
                        elif x == 1:
                            row_ann.append("1.00")
                        else:
                            row_ann.append("0")
                    annotations.append(row_ann)

                # Get display labels (shorter versions) for the heatmap
                display_labels = []
                for label in label_list:
                    if label in LABEL_DISPLAY_NAMES[task]:
                        display_labels.append(LABEL_DISPLAY_NAMES[task][label])
                    else:
                        # If no display name is defined, use the original label
                        display_labels.append(label)

                # Plot the heatmap
                sns.heatmap(
                    custom_norm,
                    annot=annotations,
                    fmt="",
                    cmap=sns.diverging_palette(10, 240, s=100, l=40, n=9),
                    xticklabels=display_labels,  # Use shorter display labels
                    yticklabels=display_labels,  # Use shorter display labels
                    ax=ax,
                    cbar=False,
                    annot_kws={"fontsize": 12},
                    vmin=-0.5,
                    center=0,
                    vmax=1.0,
                )

                # Prepare title with accuracy if available
                title = f"{model_display}: {task_titles[task]}"
                if accuracy is not None:
                    title += f" (Acc: {accuracy:.1f}%)"

                # Labels and title
                ax.set_title(title, fontsize=12)
                ax.set_xlabel("Predicted", fontsize=11)
                ax.set_ylabel("True", fontsize=11)
                ax.set_xticklabels(
                    ax.get_xticklabels(), rotation=30, ha="right", fontsize=10
                )
                ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=10)

            except Exception as e:
                ax.text(
                    0.5,
                    0.5,
                    f"Error: {str(e)}",
                    ha="center",
                    va="center",
                    fontsize=10,
                    wrap=True,
                )
                ax.set_title(f"{model_display}: {task_titles.get(task, task)} (Error)")
                ax.axis("off")

    plt.tight_layout()
    if "llm_model" in df.columns and not df["llm_model"].isna().all():
        # Add a bit more space at the top for the title
        plt.subplots_adjust(top=0.93)

    # -------------------------------------------------------------------------
    # 8) Print the figure in the notebook
    # -------------------------------------------------------------------------
    plt.show()
